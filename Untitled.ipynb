{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541e8352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb6efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dateset = Dataset.from_dict(train_data)\n",
    "dev_dataset = Dataset.from_dict(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36549b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346e3da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25887431fe0f43879ed030e68382c87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ecd5b9ef4e46e8a32ac803cbeea3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb84b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59cc0887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7590810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4b7ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b38fe42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb650335fc59407c969595ae3f507a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2972, 'learning_rate': 2.563235294117647e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Martin\\Desktop\\projects\\dialogue text summarizaiton\\txt_sum\\Lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1053cebc12a47dc9e8aaedfaf07bb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43505340814590454, 'eval_rouge1': 0.4446, 'eval_rouge2': 0.2173, 'eval_rougeL': 0.3797, 'eval_rougeLsum': 0.3799, 'eval_gen_len': 18.7602, 'eval_runtime': 38.3734, 'eval_samples_per_second': 34.347, 'eval_steps_per_second': 1.095, 'epoch': 1.0}\n",
      "{'loss': 0.4891, 'learning_rate': 2.122058823529412e-05, 'epoch': 1.18}\n",
      "{'loss': 0.457, 'learning_rate': 1.6808823529411764e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Martin\\Desktop\\projects\\dialogue text summarizaiton\\txt_sum\\Lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6eb1480a99a46eea01da98b0cb533c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4185820519924164, 'eval_rouge1': 0.4525, 'eval_rouge2': 0.2298, 'eval_rougeL': 0.3867, 'eval_rougeLsum': 0.3871, 'eval_gen_len': 18.7026, 'eval_runtime': 69.445, 'eval_samples_per_second': 18.979, 'eval_steps_per_second': 0.605, 'epoch': 2.0}\n",
      "{'loss': 0.4375, 'learning_rate': 1.2397058823529411e-05, 'epoch': 2.35}\n",
      "{'loss': 0.4252, 'learning_rate': 7.985294117647059e-06, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Martin\\Desktop\\projects\\dialogue text summarizaiton\\txt_sum\\Lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc863da9b6ee4ab68d939fb38bc2fe6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41470426321029663, 'eval_rouge1': 0.4573, 'eval_rouge2': 0.2334, 'eval_rougeL': 0.3903, 'eval_rougeLsum': 0.3905, 'eval_gen_len': 18.8103, 'eval_runtime': 76.1924, 'eval_samples_per_second': 17.298, 'eval_steps_per_second': 0.551, 'epoch': 3.0}\n",
      "{'loss': 0.4092, 'learning_rate': 3.573529411764706e-06, 'epoch': 3.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Martin\\Desktop\\projects\\dialogue text summarizaiton\\txt_sum\\Lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09808f24813a413180a9a546f43e7527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4142823815345764, 'eval_rouge1': 0.4563, 'eval_rouge2': 0.234, 'eval_rougeL': 0.3898, 'eval_rougeLsum': 0.39, 'eval_gen_len': 18.9226, 'eval_runtime': 78.4689, 'eval_samples_per_second': 16.796, 'eval_steps_per_second': 0.535, 'epoch': 4.0}\n",
      "{'train_runtime': 7605.5552, 'train_samples_per_second': 14.301, 'train_steps_per_second': 0.447, 'train_loss': 0.5646691939410041, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3400, training_loss=0.5646691939410041, metrics={'train_runtime': 7605.5552, 'train_samples_per_second': 14.301, 'train_steps_per_second': 0.447, 'train_loss': 0.5646691939410041, 'epoch': 4.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "074c62a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = 'dialogue_summ_model/checkpoint-3000/'\n",
    "summarizer = pipeline(\"summarization\", model=model_path, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f73214cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Agent: Hello, how can I help you? Customer: Hi! I need to return a pair of shoes I ordered. \" + \\\n",
    "       \"The order number is #10120. Agent: Sure, I can help you. Why do you want to return the shoes? \" + \\\n",
    "       \"Customer: They didn't fit :( But I will for something else. Agent: Understandable. Just fill in this form\" + \\\n",
    "       \", put the shoes back in the box and send them back. That's all.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18043f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will: hey babe, what do you want for dinner tonight?\n",
      "Emma:  gah, don't even worry about it tonight\n",
      "Will: what do you mean? everything ok?\n",
      "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
      "Will: Well what time will you be home?\n",
      "Emma: soon, hopefully\n",
      "Will: you sure? Maybe you want me to pick you up?\n",
      "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
      "Will: Alright, love you. \n",
      "Emma: love you too. \n"
     ]
    }
   ],
   "source": [
    "print(test_data['document'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6901879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Will will pick Emma up when he gets home.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(test_data['document'][3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
